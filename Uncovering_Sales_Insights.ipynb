{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CFIxATuL_ij"
      },
      "source": [
        "# **Uncovering Sales Insights using Frequent Itemset Mining and Association Rules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHBBd8nJGYGY"
      },
      "source": [
        "Initially, our project focused on Real-Time Demand Forecasting in the E-commerce sector.\n",
        "However, the dataset's size was limited, We didnt had a lot of different sales recoreds and conventional models like Random Forest couldn't handle the vast number of different items present.\n",
        "Consequently, we rephrased our objective to center around \"Uncovering Sales Insights using Frequent Itemset Mining and Association Rules in PySpark\".\n",
        "\n",
        "Insufficient bill numbers continue to hinder accurate predictions and comprehensive insights.\n",
        "Increasing the dataset size with more bills remains a critical objective for improvement.\n",
        "\n",
        "\n",
        "Another dataset with more bills became available, but it came with a new challenge.\n",
        "The number of different items in this dataset was significantly larger, approximately ten times more than before.\n",
        "This created a problem due to the ratio remaining the same between the number of bills and the number of items.\n",
        "As a result, the issue of sparsity persisted, making it difficult to uncover meaningful patterns and associations between items.\n",
        "Balancing the dataset to overcome this problem has become a priority to derive valuable insights effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FECquyv--uKJ"
      },
      "source": [
        "## We used several articles.\n",
        "\n",
        "#### 1. \"Educational Data Mining with Python and Apache Spark: A Hands-on Tutorial\" - From Afeka Model\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "We used the following methodologies in order to reach the solution of our problem from the creation of the data mining to the creation of the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 2. \"An optimized FP-growth algorithm for discovery of association rules\" - <link>https://link.springer.com/article/10.1007/s11227-021-04066-y<link/>\n",
        "\n",
        "The article talks about the FP-growth model.\n",
        "Quote from the article \"Association rule mining (ARM) is a data mining technique to discover interesting associations between datasets.\"\n",
        "\n",
        "We chose to use this model in order to predict the question being asked.\n",
        "\n",
        "\n",
        "#### 3.\"Big data: Some statistical issues\" - <link>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5992743/<link/>\n",
        "An article that talks about how to work with imperfect data such as null or incorrect information in cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAhmQ3WsyGqi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhbEwiKayEra",
        "outputId": "2890d1f3-1797-4417-eab5-e825f7885ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /home/linuxu/anaconda3/lib/python3.9/site-packages (3.4.1)\n",
            "Requirement already satisfied: kafka-python in /home/linuxu/anaconda3/lib/python3.9/site-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /home/linuxu/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /home/linuxu/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/linuxu/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/linuxu/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /home/linuxu/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /home/linuxu/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Python libraries\n",
        "!pip install pyspark kafka-python pandas\n",
        "\n",
        "from kafka import KafkaConsumer, KafkaProducer\n",
        "from pyspark.sql.functions import collect_set\n",
        "import pandas as pd\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.sql.window import Window\n",
        "import sys\n",
        "from pyspark.ml.fpm import FPGrowth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdkCJOq99iP4"
      },
      "source": [
        "# Kafka Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I3qJzxH88Ou",
        "outputId": "65c5cfed-29a4-44ce-e4b0-deb36fd47b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic Market_Basket1 created\n"
          ]
        }
      ],
      "source": [
        "from confluent_kafka.admin import AdminClient, NewTopic\n",
        "\n",
        "# Create an admin client\n",
        "admin_client = AdminClient({'bootstrap.servers': 'localhost:9092'})\n",
        "\n",
        "# Define a new topic\n",
        "new_topics = [NewTopic(topic, num_partitions=1, replication_factor=1) for topic in ['Market_Basket']]\n",
        "\n",
        "# Create new topic\n",
        "fs = admin_client.create_topics(new_topics)\n",
        "\n",
        "# Wait for each operation to finish\n",
        "for topic, f in fs.items():\n",
        "    try:\n",
        "        f.result()  # The result itself is None\n",
        "        print(\"Topic {} created\".format(topic))\n",
        "    except Exception as e:\n",
        "        print(\"Failed to create topic {}: {}\".format(topic, e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uv70F5N88Ou",
        "outputId": "55ba8376-5ee0-437f-93ab-f6da1181cf31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Function to convert Timestamp to string\n",
        "def timestamp_to_string(obj):\n",
        "    if isinstance(obj, pd.Timestamp):\n",
        "        return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    return obj\n",
        "\n",
        "# Create a producer\n",
        "producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('Assignment-1_Data.xlsx')\n",
        "\n",
        "# Convert Timestamps to string\n",
        "df = df.applymap(timestamp_to_string)\n",
        "\n",
        "# Send each row to Kafka\n",
        "for index, row in df.iterrows():\n",
        "    producer.send('Market_Basket', value=row.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q8XEbnc9ye-"
      },
      "source": [
        "## Kafka Consumer\n",
        "This Consumer was used to test the producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpQIswx288Ov"
      },
      "outputs": [],
      "source": [
        "# # Create a consumer that starts from the earliest available message\n",
        "# consumer = KafkaConsumer('Market_Basket', bootstrap_servers='localhost:9092', auto_offset_reset='earliest', value_deserializer=lambda v: json.loads(v.decode('utf-8')))\n",
        "\n",
        "# # Print each message\n",
        "# for msg in consumer:\n",
        "#     print(msg.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDKgiffs-nrN"
      },
      "source": [
        "# Kafka Integration with PySpark for Structured Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09uJTW5m88Ow"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Customer Purchasing Patterns\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema of the incoming data\n",
        "schema = StructType([\n",
        "    StructField(\"BillNo\", StringType()),\n",
        "    StructField(\"Itemname\", StringType()),\n",
        "    StructField(\"Quantity\", IntegerType()),\n",
        "    StructField(\"Date\", TimestampType()),\n",
        "    StructField(\"Price\", FloatType()),\n",
        "    StructField(\"CustomerID\", FloatType()),\n",
        "    StructField(\"Country\", StringType())\n",
        "])\n",
        "\n",
        "# Create a DataFrame from the Kafka stream\n",
        "df = spark \\\n",
        "    .read \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"Market_Basket\") \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .option(\"endingOffsets\", \"latest\") \\\n",
        "    .load() \\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
        "    .select(\"data.*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXz0CFQj_KMU"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dmcm1irIlFJ"
      },
      "source": [
        "Removes rows with null 'Itemname'.\n",
        "\n",
        "Filters out rows with non-positive 'Quantity' - Some of the rows have negative numbers and no product name. These lines have no meaning for what we want to do.\n",
        "\n",
        "Fills missing 'CustomerID' within the same 'BillNo' and Generates unique 'CustomerID' for bills without one.\n",
        "\n",
        "Converts 'Itemname' to numeric indices using StringIndexer.\n",
        "\n",
        "Drops irrelevant columns 'Itemname' and 'Country'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTLD6bSB88Ow",
        "outputId": "f7b4e2ef-6e0a-4a8b-f5a5-79c6fb694fdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Drop all the rows that has null in Itemname\n",
        "df = df.na.drop(subset=['Itemname'])\n",
        "\n",
        "# Remove rows where 'Quantity' is less than or equal to 0\n",
        "df = df.filter(df['Quantity'] > 0)\n",
        "\n",
        "# Fill the CustomerID within the same BillNo\n",
        "df = df.withColumn('CustomerID', F.last('CustomerID', True).over(Window.partitionBy('BillNo').orderBy('Date').rowsBetween(-sys.maxsize, 0)))\n",
        "\n",
        "# For BillNo without a CustomerID, generate a unique ID\n",
        "df = df.withColumn('CustomerID', F.when(F.col('CustomerID').isNull(), F.concat(F.lit('cust'), F.monotonically_increasing_id())).otherwise(F.col('CustomerID')))\n",
        "\n",
        "# Using StringIndexer to index categorical columns (Itemname): this assigns one numeric index to each different string in the column\n",
        "indexer_item = StringIndexer(inputCol=\"Itemname\", outputCol=\"ItemnameIndex\").setHandleInvalid(\"keep\")\n",
        "\n",
        "df = indexer_item.fit(df).transform(df)\n",
        "\n",
        "df1 = df.drop(\"Itemname\",\"Country\") # Delete Country because most of the values are the UK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4FvjT2bAN4_"
      },
      "source": [
        "**Group the different Items in each bill by \"BillNo\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaO4Y2NL88Ox"
      },
      "outputs": [],
      "source": [
        "# Preprocess data into transactions, where each transaction is a set of items\n",
        "\n",
        "baskets = df.groupBy(\"BillNo\").agg(collect_set('ItemnameIndex').alias('Items'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4f1DRcz88Ox",
        "outputId": "9832ac2c-0952-40e0-eb1a-c8c1896977b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "|BillNo|               Items|\n",
            "+------+--------------------+\n",
            "|536366|     [184.0, 2900.0]|\n",
            "|536367|[251.0, 3436.0, 2...|\n",
            "|536371|              [13.0]|\n",
            "|536374|            [1023.0]|\n",
            "|536375|[0.0, 515.0, 37.0...|\n",
            "|536377|     [184.0, 2900.0]|\n",
            "|536384|[42.0, 9.0, 501.0...|\n",
            "|536385|[23.0, 235.0, 353...|\n",
            "|536386| [41.0, 1829.0, 1.0]|\n",
            "|536387|[1620.0, 1263.0, ...|\n",
            "|536389|[2379.0, 330.0, 2...|\n",
            "|536395|[238.0, 35.0, 161...|\n",
            "|536396|[0.0, 515.0, 37.0...|\n",
            "|536398|[68.0, 185.0, 363...|\n",
            "|536399|     [184.0, 2900.0]|\n",
            "|536403|       [25.0, 149.0]|\n",
            "|536404|[799.0, 857.0, 12...|\n",
            "|536407|     [184.0, 2900.0]|\n",
            "|536412|[799.0, 38.0, 469...|\n",
            "|536414|             [132.0]|\n",
            "+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "baskets.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62o2ja3z88Oy",
        "outputId": "b4dc901e-ac69-40f9-ced3-f26fc0d4fca8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 12:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of DataFrame (rows, columns): (20327, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Get the number of rows\n",
        "num_rows = baskets.count()\n",
        "\n",
        "# Get the number of columns\n",
        "num_columns = len(baskets.columns)\n",
        "\n",
        "# Calculate the size in terms of rows and columns\n",
        "size_in_rows_and_columns = (num_rows, num_columns)\n",
        "print(\"Size of DataFrame (rows, columns):\", size_in_rows_and_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EM0xCqZAzrq"
      },
      "source": [
        "# Train The Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik2HgxFJE6FH"
      },
      "source": [
        "\n",
        "The FPGrowth algorithm is a frequent itemset mining algorithm used for discovering interesting associations between items in a transactional dataset. It works by identifying sets of items that frequently occur together in transactions. We use FPGrowth because it efficiently handles large datasets and can uncover meaningful patterns without the need for candidate itemset generation, making it faster than traditional Apriori-based approaches.\n",
        "\n",
        "Frequent itemset mining is valuable in various domains, including E-commerce. It helps businesses understand customer buying behavior, uncover item correlations, and make data-driven decisions. By using the FPGrowth algorithm, we can efficiently extract association rules that reveal co-occurring item patterns in the dataset, providing valuable insights for improving recommendation systems, marketing strategies, and overall business operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZq-DVqvFjeQ"
      },
      "source": [
        "minSupport=0.01, minConfidence=0.5\n",
        "\n",
        "\n",
        "The low minSupport value (0.01) is chosen because the dataset is not very big. This setting allows capturing more itemsets that occur somewhat frequently in the data and helps uncover potentially valuable insights and associations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WANNH4th88Ox",
        "outputId": "a339f467-9c32-46af-970d-b278b8c9cd5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+----+\n",
            "|         items|freq|\n",
            "+--------------+----+\n",
            "|       [144.0]| 554|\n",
            "| [144.0, 85.0]| 230|\n",
            "| [144.0, 99.0]| 210|\n",
            "|       [559.0]| 264|\n",
            "|       [224.0]| 451|\n",
            "|       [567.0]| 256|\n",
            "|        [95.0]| 681|\n",
            "|       [771.0]| 204|\n",
            "|       [414.0]| 311|\n",
            "|[414.0, 201.0]| 215|\n",
            "|       [194.0]| 471|\n",
            "|       [525.0]| 277|\n",
            "|       [186.0]| 502|\n",
            "| [186.0, 88.0]| 206|\n",
            "|       [260.0]| 421|\n",
            "|       [717.0]| 216|\n",
            "|       [745.0]| 211|\n",
            "|       [431.0]| 320|\n",
            "|       [510.0]| 286|\n",
            "|       [170.0]| 523|\n",
            "+--------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+----------+------------------+------------------+--------------------+\n",
            "|      antecedent|consequent|        confidence|              lift|             support|\n",
            "+----------------+----------+------------------+------------------+--------------------+\n",
            "|   [132.0, 50.0]|    [36.0]|0.7684887459807074|16.204430227748794|0.011757760613961726|\n",
            "|    [52.0, 12.0]|    [14.0]|0.6493150684931507|11.319577527667473|0.011659369311752842|\n",
            "|    [52.0, 12.0]|    [10.0]|0.6082191780821918|10.209142223680193|0.010921434545186206|\n",
            "|    [52.0, 12.0]|     [1.0]|0.7342465753424657| 7.231119252415843|0.013184434495990554|\n",
            "|     [18.0, 1.0]|    [88.0]|0.6721311475409836| 19.29718903399092|0.010085108476410686|\n",
            "|[19.0, 8.0, 4.0]|    [16.0]|0.6386292834890965|11.467683255726913|0.010085108476410686|\n",
            "|     [33.0, 2.0]|    [71.0]|0.6694386694386695|18.488695426195427|0.015840999655630444|\n",
            "|     [33.0, 2.0]|    [30.0]|0.7900207900207901|15.852667915846594|  0.0186943474196881|\n",
            "|     [58.0, 4.0]|    [28.0]| 0.742671009771987|14.685091065792975| 0.01121660845181286|\n",
            "|     [58.0, 4.0]|    [48.0]|0.6905537459283387|16.042155421126104|0.010429478034141782|\n",
            "|     [31.0, 8.0]|    [22.0]|0.5522727272727272|10.462299839023977|0.011954543218379495|\n",
            "|     [31.0, 8.0]|     [4.0]|0.6454545454545455| 8.514052268302756|0.013971564913661633|\n",
            "|     [31.0, 8.0]|    [15.0]|0.5886363636363636| 10.43174486803519|0.012741673636050574|\n",
            "|     [31.0, 8.0]|    [19.0]|              0.55| 10.37091836734694|0.011905347567275053|\n",
            "|     [31.0, 8.0]|    [16.0]|0.5727272727272728|10.284299710889817|0.012397304078319477|\n",
            "|    [84.0, 10.0]|     [1.0]| 0.770392749244713| 7.587099522237055|0.012544891031632803|\n",
            "|         [327.0]|   [118.0]|0.5956284153005464|20.012130244321003|0.010724651940768435|\n",
            "|    [33.0, 30.0]|    [71.0]|0.7045143638850889| 19.45742319931006|0.025335760318787818|\n",
            "|    [33.0, 30.0]|     [2.0]|0.5198358413132695| 5.549739047465772|  0.0186943474196881|\n",
            "|         [187.0]|   [135.0]|             0.552| 19.11499829642249|0.013577999704826094|\n",
            "+----------------+----------+------------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 610:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+--------------------+\n",
            "|BillNo|               Items|          prediction|\n",
            "+------+--------------------+--------------------+\n",
            "|536366|     [177.0, 2760.0]|                  []|\n",
            "|536367|[251.0, 5.0, 269....|      [430.0, 278.0]|\n",
            "|536371|              [13.0]|                  []|\n",
            "|536374|            [1016.0]|                  []|\n",
            "|536375|[0.0, 37.0, 20.0,...|                  []|\n",
            "|536377|     [177.0, 2760.0]|                  []|\n",
            "|536384|[42.0, 9.0, 104.0...|                  []|\n",
            "|536385|[234.0, 21.0, 93....|[1.0, 12.0, 15.0,...|\n",
            "|536386| [41.0, 1821.0, 1.0]|  [14.0, 10.0, 12.0]|\n",
            "|536387|[1612.0, 1092.0, ...|                  []|\n",
            "|536389|[35.0, 388.0, 234...|              [46.0]|\n",
            "|536395|[238.0, 138.0, 35...|       [103.0, 53.0]|\n",
            "|536396|[0.0, 37.0, 20.0,...|                  []|\n",
            "|536398|[138.0, 68.0, 104...|             [149.0]|\n",
            "|536399|     [177.0, 2760.0]|                  []|\n",
            "|536403|       [25.0, 149.0]|      [138.0, 103.0]|\n",
            "|536404|[580.0, 318.0, 22...|[19.0, 4.0, 15.0,...|\n",
            "|536407|     [177.0, 2760.0]|                  []|\n",
            "|536412|[1942.0, 1421.0, ...|[19.0, 22.0, 31.0...|\n",
            "|536414|             [131.0]|                  []|\n",
            "+------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# train FPGrowth model\n",
        "fpGrowth = FPGrowth(itemsCol=\"Items\", minSupport=0.01, minConfidence=0.5)\n",
        "model = fpGrowth.fit(baskets)\n",
        "\n",
        "# Display frequent itemsets\n",
        "model.freqItemsets.show()\n",
        "\n",
        "# Display generated association rules\n",
        "model.associationRules.show()\n",
        "\n",
        "# transform examines the input items against all the association rules and summarize the\n",
        "# consequents as prediction\n",
        "model.transform(baskets).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d7_a-9DAxPH"
      },
      "source": [
        "As we can see, the model has returned the most frequent items and their number of occurrences.\n",
        "\n",
        "Also, we have the different items (consequent) and their chance to be in the same cart with one of the antecedent items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S-a6YGGKp_Z"
      },
      "source": [
        "# Old question and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dn18nYb88Oy"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql import Window\n",
        "# from pyspark.sql.functions import lag, avg, dayofmonth, month, dayofweek\n",
        "\n",
        "# # Define a window specification\n",
        "# windowSpec = Window.partitionBy('ItemnameIndex').orderBy('Day', 'Month')\n",
        "\n",
        "# # Past sales\n",
        "# df3 = df2.withColumn('PrevDaySales', lag(df2['Quantity']).over(windowSpec))\n",
        "\n",
        "# # Lag features (sales 7 days ago)\n",
        "# df4 = df3.withColumn('Sales7DaysAgo', lag(df3['Quantity'], 7).over(windowSpec))\n",
        "\n",
        "# # Rolling window statistics (mean sales over the past week)\n",
        "# df5 = df4.withColumn('RollingWeekSalesMean', avg(df4['Quantity']).over(windowSpec.rowsBetween(-6, 0)))\n",
        "\n",
        "# # Time indicators\n",
        "# df6 = df5.withColumn('DayOfWeek', dayofweek(df5['Date']))\n",
        "\n",
        "# # Note: Make sure to handle missing values that may have been introduced by creating these features\n",
        "# df6 = df6.na.fill(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hwg5K2T88Ox"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql.functions import month, dayofmonth\n",
        "\n",
        "# df2 = df1.withColumn('Day', dayofmonth(df1['Date']))\n",
        "# df2 = df2.withColumn('Month', month(df2['Date']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJwDbu3v88Ox"
      },
      "outputs": [],
      "source": [
        "#df3 = df2.drop(\"Date\")\n",
        "#df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd5S0vUCKdqN"
      },
      "outputs": [],
      "source": [
        "# from pyspark.ml.recommendation import ALS\n",
        "# from pyspark.ml.evaluation import RegressionEvaluator\n",
        "# from pyspark.sql.functions import lit\n",
        "\n",
        "# # Convert 'Day' and 'Month' into a single 'Date' column\n",
        "# # You will need to adjust this based on how your 'Day' and 'Month' columns are represented\n",
        "# df = df.withColumn('Date', (df['Month'] - 1) * 30 + df['Day'])\n",
        "\n",
        "# # Split the data into training and test sets\n",
        "# train_data, test_data = df.randomSplit([0.8, 0.2])\n",
        "\n",
        "# # Define the ALS model\n",
        "# als = ALS(\n",
        "#     userCol='Date',\n",
        "#     itemCol='ItemNameIndex',\n",
        "#     ratingCol='Quantity',\n",
        "#     coldStartStrategy='drop'\n",
        "# )\n",
        "\n",
        "# # Train the model with the training data\n",
        "# model = als.fit(train_data)\n",
        "\n",
        "# # Make predictions on the test data\n",
        "# predictions = model.transform(test_data)\n",
        "\n",
        "# # Evaluate the model\n",
        "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Quantity\", predictionCol=\"prediction\")\n",
        "# rmse = evaluator.evaluate(predictions)\n",
        "# print(f\"Root-mean-square error = {rmse}\")\n",
        "\n",
        "# # Make predictions for the next day\n",
        "# next_day = train_data.agg({\"Date\": \"max\"}).collect()[0][0] + 1\n",
        "# next_day_df = df.select('ItemNameIndex').distinct().withColumn('Date', lit(next_day))\n",
        "# next_day_predictions = model.transform(next_day_df)\n",
        "\n",
        "# # Select the top 10 items with the highest predicted quantities\n",
        "# top10_items = next_day_predictions.sort('prediction', ascending=False).limit(10).select('ItemNameIndex').collect()\n",
        "\n",
        "# # 'top10_items' is a list of Row objects, so let's extract the item indices\n",
        "# top10_item_indices = [row['ItemNameIndex'] for row in top10_items]\n",
        "\n",
        "# print(f\"Top 10 recommended items for day {next_day}: {top10_item_indices}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}